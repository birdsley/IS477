For our final project, we have been exploring the links between asthma and poor air qualities by county. We identified two different data sets, one that contains detailed air quality information by county, and another that has asthma prevalence by county. We decided to offset the data sets by two years since poor air quality may take time to come into effect. By examining these two data sources in tandem, we aim to determine if there is a strong correlating effect between the two data sets. 

We first began by creating a way to retrieve the data that did not involve manually downloading anything. For the first data source, the air quality data, we were able to query the url from the Environmental Protection Agency and download the original .zip file that we first retrieved when we were exploring different data sets. Since it was retrieved in a .zip format, we had to extract the actual table. This involved creating an ‘if’ statement within our retrieval function, which can be found in the 01_data_acquisition file in the ‘scripts’ folder of our GitHub repository. If someone wanted to change the year or exact table that is pulled, all they would have to do is alter the url string at the bottom of the script. Although, this may cause potential problems later if the new table contains a different schema from the annual 2018 table.

For our second data set, we retrieved asthma prevenance data for each county from the Center for Disease Control. This data was accessible by a specific data resource ID, ‘mssc-ksj7’. By plugging this into the end of link that brings you to the series of tables, the csv file will automatically be retrieved. A potential improvement for this process could be saving the URLs in a .txt file and having the script reference this. That way, you would not have to alter the actual script itself but rather the text file that is being referenced. However, these ur’'s are not confidential, unlike an API key, so we have not yet implemented this feature.

Next, we wanted to ensure that our data was reproducible and had a way to verify that the raw data had not been modified. This is important for someone else who might come across our work and wants to use it. If so, they need a way to verify that they are working with the same source of truth as we did. By utilizing a SHA-256 checksum verification, new users can be assured of this. Our code finds the checksum for each of the data sources and saves these as hexadecimal strings in JSON format. These files are then stored in a ‘checksum’ folder within the ‘data’ branch.

After checking the raw files, the next step in our project was loading both datasets into a spot where we could work with them easier without worrying about file types or anything getting messed up. We used a small SQLite database for this since it don’t really need any setup and works fine for our size of data. In the 03_load_to_db.py script, we create the health_air_quality.db file and store the EPA table as air_quality and the CDC table as asthma. This helped keep stuff in one place and made it faster to look at things or test joins before moving on. The csv files loaded came straight from data/raw, so everything stays repeatable.

After this we worked on joining the two tables into one dataset we could use for analysis. At first, nothing matched because the counties and states were written differently in each dataset. The EPA one used “County” and “State”, and the CDC one used “countyname” and “statedesc”, plus different spaces and caps. So in 04_integrate_data.py, we cleaned both by making everything lowercase and stripping extra spaces. Once that was done, we used an inner join and ended up with 327 matching counties. We saved that as asthma_air_quality_merged.csv inside data/processed. That completed the main tasks we planned for this part.

For the timeline, most tasks stayed close to what we wrote before even if we shifted a few parts around. The download and checksum steps were finished early. Loading the data into the database took a bit more time just because we had to test the CDC file more, but it didn’t really push anything back. The merge step was delayed a bit because we moved the checksum step earlier in the plan. Now the next tasks are exploring the merged data, checking the patterns, and then making the final charts and writing up what the relationship between air quality and asthma looks like. We should get this done before the last deadline.

We also changed our project plan after the most recent comments. One big thing was adding the SHA 256 check since we didn’t show how we would keep the raw data stable before. Adding that made the workflow more trustable. We also updated the plan to include the name cleaning step because the datasets really didn’t match as easily as we first thought. 


For my (Sam Birdsley) contributions, I have worked to integrate the data acquisition and verification process to make it as smooth as possible. With the first two scripts running off each other and saving all outputs in specific folders of the ‘data’ branch, any user can quickly have the data and be assured that it has not been altered. Along with these scripts, I also documented all of this in the ‘README’ file in the main branch of the repository. I talked about the source of the data and any important caveats, before I documented the schemas for both data sets. This is especially important since some of the columns can have arbitrary names, so thorough documentation is crucial for both sources. Finally, I also included documentation on the data integrity check and where the hexadecimals are saved for verification.

For my (Michael Strobl) contributions, I mainly handled the 03_load_to_db.py and 04_integrate_data.py scripts. I set up the database loading so the EPA and CDC files save clean into SQLite and tested it a few times since the CDC file acted a bit weird at first. I also worked on the data merge, fixing county and state names so they matched, and then creating the final joined file with the 327 counties. This sets up the main data set we will use going forward.
